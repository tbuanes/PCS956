{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Grouping loss\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the ML algorithm is well calibrated overall, it is not guaranteed that the confidence for individual instances is well matched with the true probability for its classification. Before attempting a more general discussion, lets consider an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip title=\"Example\"}\n",
    "The [CIFAR-10](https://paperswithcode.com/dataset/cifar-10) dataset consists of 60000 (training) + 10000 (test) 32x32 pixel colour images organised into 10 non-overlapping categories. Using a quite small NN, the images are categorised with 71% accuracy. To check how well calibrated the NN is, we plot the fraction of correctly classified images as a function of NN output (20 bins). With a perfectly calibrated NN, the fraction should correspond to NN output, i.e. all points should be on the dashed line. The size of error bars in the plots are calculated assuming a Poisson distribution (which we will se is not appropriate in this example)\n",
    "\n",
    "![](./images/cifar10_calibration.png)\n",
    "\n",
    "Except for the three first bins, which are empty, the NN agreement with the dashed line is very good. We could therefore be tempted to conclude that this is a well-calibrated NN and move on. But things are not so simple. Even though the NN is well calibrated *on average*, this does not need to be the case for each individual prediction. Thus even though an 70% of all images with output 0.7 indeed are correctly classified, it may be - and is in this case - subsets of images with NN output 0.7 where the fraction of correctly classified images deviate a lot from 70%. \n",
    "\n",
    "If we now make the same plot, but restricted to data belonging to only one category things look different. For instance classification of images which really belongs to category 5 (dogs) still look pretty good, but for category 3 (cats) the fraction of correctly classified images is much lower than the \"probability\" found by the NN.\n",
    "\n",
    "![](./images/cifar_calibration_cat5.png){width=40%} ![](./images/cifar_calibration_cat3.png){width=40%}\n",
    "\n",
    "Making the same plot for all 10 categories (hiding the errorbars to de-clutter the plot) shows that the spread is quite large. Thus, even though the NN output on average had a good match to the probability for the image to be classified correctly, on an category by category basis this is far from true. What hasn't been checked here, but is conceivable, is that there are also variations within the categories. It could for instance be that only a subset of the cat images are classified with too low confidence, while another subset has correct or even too high confidence.\n",
    "\n",
    "![](./images/cifar_calibration_allcat.png)\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
