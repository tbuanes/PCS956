{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Loss function, metric and utility\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function, metric and utility are related, but still different concepts that are used when developing a machine learning model. The former two are best known, and are generally discussed in any introduction to machine learning. The latter is less often discussed, but should not be forgotten when creating a machine learning model which is to be used for decision making. The same mathematical function may be used for all three, but more typically one will use different functions for each of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The role of the **loss function** is to measure the performance of the machine learning model during training, to guide the further training process. For example in the case of supervised learning, the loss function is a quantitative measure of the difference between the machine learning models prediction on (a batch of) the training data and the ground truth of the same dataset. Thus the training proceeds by trying to minimize the loss function. In order for a loss function to work well in the optimisation procedure it needs to be as close to continous as possible. To understand why, it is easiest to first consider a counter-example. Let's say we use the precision,\n",
    "$$\\text{Precision} = \\frac{TP}{TP+TN}$$\n",
    "as loss function in a classification problem.  Here *TP* and *TN* are the number of true positives (instances correctly classified to the class we denote as positive) and true negatives (instances correctly classified to the class we denote negative), respectively. For this proposed loss function to change its value, the parameters of the model must be changed enough to change the classification of at least one event, thus increasing or decreasing TP or TN. In many cases this may require a substantial change in parameters, causing the training process to stop prematurely. A more commonly used, and better loss function for classification is the cross entropy\n",
    "$$H = -y\\log\\hat{y} - (1-y)\\log(1-\\hat{y})$$\n",
    "where $y = 0 \\text{ or } 1$ denominates the true class of the instance, whereas $0\\leq \\hat{y} \\leq 1$ is the model's prediction of the same instance. Comparing to the Precision case above, the value of $\\hat{y}$ would have to cross a threshold (ofte taken to be $0.5$) for the loss function to change its value. However, in this case any change in the value of $\\hat{y}$ would change the value of the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "The role of the **metric** is to measure the performance of the trained machine learning model, and to allow for comparison of the performance across different models applied to the same task. The metric should not just by any number which can measure the performance, but it should ideally be easy to interpret the meaning of its numerical value. Furthermore, the measure should be chosen to be meaningful for the type of data, and in the relevant context the machine learning model is applied to. This latter point will be the topic of the next section, and will thus not be discussed here. But it can be useful to briefly look at why a typical loss function such as cross entropy is normally not well suited as a metric. After all, the loss function is also a meaure of the model's performance, so one might wonder why not let us also be our metric. First of all, the cross entropy as defined above, was presented per evaluated instance. Thus to evaluate the performance on a larger batch of instances, one would sum over all individual contributions. This means that the value of the cross entropy depends on the batch size, so one would at least have to normalise it to get a meaningful value for comparison. But the cross entropy also depends on the exact values of $\\hat{y}$ from the model. Two models which perform equally well measured by other means can have very different $\\hat{y}$-distributions. Thus for a comparison across different model architectures, the cross entropy (and most other typical loss functions) is not well suited. Furthermore, different types of machine learning models (neural networks, support vector machines, regression trees, etc.) may have different optimal, or even possible, loss functions - again showing that the loss function is usually not a good choise as metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility  {#sec-utility}\n",
    "The **utility** measures the \"value\" of the outcome of the decision made by the machine learning model; for instance how much money is won or lost by making that decision. Thus the decision process should be set up such that the expected utility is maximised. Maximising the utility is not necessarily equivalent to optimising one of the metrics which is typically used for evaluating model performance. \n",
    "\n",
    ":::{.callout-tip title=\"Example\"}\n",
    "As an example of this, let's imagine a test that determines whether maintenance of some machinery is required. There is a cost to performing maintenance, but failing to maintain the machinery when needed will cause a breakdown that is much more expensive. Define *positive = test claims that maintenance is required* and *negative = test claims that maintenance is not required*. Assume that the associated costs are (in some arbitrary units):\n",
    "\n",
    "| TP | FP | TN |  FN  |\n",
    "|----|----|----|------|\n",
    "| -1 | -1 |  0 |-1000 |\n",
    "\n",
    "Furthermore, let's assume that at some particular time the probability that maintenance is in fact required is 5%. \n",
    "\n",
    "If we assume our test to be 97% accurate, we can calculate the expected cost as:\n",
    "    $$\\begin{align*}\n",
    "        U &= \\text{ACC}\\cdot(f_\\text{pos}\\cdot U_\\text{TP} + f_\\text{neg}\\cdot U_\\text{TN}) + (1-\\text{ACC})\\cdot(f_\\text{neg}\\cdot U_\\text{FP} + f_\\text{pos}\\cdot U_\\text{FN}) \\\\\n",
    "        &= 0.97\\cdot [0.05\\cdot (-1) + 0.95\\cdot 0] + 0.03\\cdot[0.95\\cdot(-1) + 0.05\\cdot(-1000)] \\\\\n",
    "        &= -1.577\n",
    "    \\end{align*}$$\n",
    "\n",
    "If one, on the other hand, we don't use the test at all and just classify everyting as positive - meaning we perform maintenance every time -  the expected cost is:\n",
    "    $$\\begin{align*}\n",
    "        U &= f_\\text{pos}\\cdot U_\\text{TP} + f_\\text{neg}\\cdot U_\\text{FP} \\\\\n",
    "        &= 0.05\\cdot (-1) + 0.95\\cdot (-1) \\\\\n",
    "        &= -1\n",
    "    \\end{align*}$$\n",
    "which is less expensive than the result using the test. So we see that the large cost of false negatives means that not even a 97% accurate model is sufficient to beat always choosing positive class. In fact, the test would need to have an accuracy of 98.1% to break even.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
